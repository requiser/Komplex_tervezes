{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bevezetés #",
   "id": "d343ea071aa2bf43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "A szakdolgozatom célja, hogy a klasszikus idősor-elemzési módszereket ötvözve a modern természetes nyelvfeldolgozás (NLP) eszköztárával, új megközelítést kínáljak a részvényárfolyamok előrejelzésére. A részvénypiacok mozgásának előrejelzése mindig is kihívást jelentett, mivel azok volatilitását számos tényező befolyásolja, legyen szó makrogazdasági eseményekről, vállalati hírekről vagy akár a befektetők érzelmeiről. Az elmúlt években a mesterséges intelligencia, különösen az NLP fejlődése új kapukat nyitott meg a pénzügyi adatelemzésben, lehetővé téve a piacot befolyásoló hírek és szöveges tartalmak automatizált feldolgozását és értelmezését.\n",
    "\n",
    "A részvényárak előrejelzésére hagyományosan alkalmazott technikai elemzés számszerű múltbeli adatokra épít. Ugyanakkor a kutatások rámutattak, hogy a kizárólag technikai elemzésre támaszkodó megközelítések nem mindig vezetnek kielégítő eredményekhez, különösen a befektetői érzelmek és a hírek hatásának figyelmen kívül hagyása miatt. A pénzügyi hírek szöveges tartalmának elemzése azonban értékes információkat nyújthat, amelyeket kiegészítve a numerikus adatokkal pontosabb előrejelzések érhetők el.\n",
    "\n",
    "A szakdolgozatom keretében különös figyelmet fordítok a BERT (Bidirectional Encoder Representations from Transformers) modell alapú NLP technikákra, amelyek lehetővé teszik a pénzügyi hírek és közlemények precíz elemzését. Az idősor-elemzés hagyományos statisztikai modelljeit pedig viszonyítási alapként használom a kísérleti eredmények kiértékeléséhez.\n",
    "\n",
    "Informatikusként hiszem, hogy a mesterséges intelligencia és az természetes nyelvfeldolgozás integrációja a pénzügyi elemzésbe nemcsak a befektetési döntések gyorsaságát és pontosságát javíthatja, hanem hozzájárulhat a piacok átláthatóságához is. Ezért választottam a részvényárfolyamok előrejelzését az NLP módszereinek segítségével szakdolgozatom témájának, hiszen ez egy olyan terület, amely az innováció és a technológiai fejlődés határmezsgyéjén helyezkedik el. Az elvégzett kutatás reményeim szerint új perspektívát nyújthat a pénzügyi elemzések világában, és hozzájárulhat a befektetési döntéshozatal hatékonyságának növeléséhez."
   ],
   "id": "2548311f459f3460"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Elméleti háttér",
   "id": "fc91b25d1c016e0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Idősorok elemzésének alapjai\n",
    "\n",
    "Az idősor-elemzés az időben egymást követő adatok elemzésének tudománya, amelyet gyakran alkalmaznak a pénzügyben, gazdasági előrejelzések készítésében, meteorológiában és számos más tudományágban. Egy idősor elemzése során az a cél, hogy azonosítsuk a megfigyelt mintázatokat, megértjük azok struktúráját, és előrejelezzük a jövőbeli értékeket. Az idősorok megértéséhez elengedhetetlen a trendek, szezonális komponensek és véletlen zajok azonosítása és elemzése.\n"
   ],
   "id": "965d7bd1ad61d16a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Az idősorok jellemzői\n",
    "\n",
    "Az idősorok különböző összetevői az adatok dinamikus viselkedését tükrözik. Ezek megértése kulcsfontosságú a helyes modellalkotáshoz és előrejelzéshez.\n",
    "\n",
    "#### Trend ($T_t$)\n",
    "\n",
    "A trend az idősor hosszú távú irányultságát jelenti, amely lehet növekvő, csökkenő vagy stagnáló. Például egy GDP-alapú gazdasági előrejelzés lehet alapul tapasztalható növekedési trendre mutat.\n",
    "\n",
    "Matematikailag a trendet egyszerű lineáris formában is leírhatjuk:\n",
    "\n",
    "$$T_t = \\beta_0 + \\beta_1 t$$\n",
    "\n",
    "ahol $\\beta_0$ az idősor kezdeti értéke, $\\beta_1$ pedig a növekedés sebességét jelenti. Amennyiben a növekedés nem lineáris, polinomiális formát alkalmazhatunk:\n",
    "\n",
    "$$T_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2$$\n",
    "\n",
    "A trendek azonosítása és modellezése kulcsfontosságú, mivel a véletlen idősor viselkedése gyakran rejt maga a rövid távú ingadozások mellett.\n",
    "\n",
    "#### Szezonalitás ($S_t$)\n",
    "\n",
    "A szezonalitás az adatok rendszeresen ismétlődő mintázatát jelenti, amelyet tipikusan az évszakok, hónapok vagy napok okoznak. Például a kiskereskedelmi forgalomban a decemberi hónap kiemelt értékeket mutat az ünnepi vásárlások miatt.\n",
    "\n",
    "A szezonalitás matematikai leírása gyakran trigonometrikus függvényekkel történik:\n",
    "\n",
    "$$S_t = A \\sin\\left(\\frac{2\\pi t}{P}\\right) + B \\cos\\left(\\frac{2\\pi t}{P}\\right)$$\n",
    "\n",
    "ahol $A$ és $B$ az amplitúdók, $P$ pedig a periódus. A szezonális hatások felismerése érdekében sokszor grafikus módszereket alkalmazunk, például a szezonális bontást, amely az idősor komponensekre bontását célozza.\n",
    "\n",
    "#### Ciklikusság ($C_t$)\n",
    "\n",
    "A ciklikus komponensek az idősorban hosszabb időközönként visszatérő mintázatokat jelentenek, amelyek nem feltétlenül szabályos időközönként ismétlődnek. A gazdasági ciklusok például ilyen ciklikus jelenségek.\n",
    "\n",
    "Egy ciklus azonosítása érdekében gyakran az autokorrelációs függvényt (ACF) használják. Az autokorrelációs függvény az idősor múltbeli értékeit és az aktuális értékeit közötti kapcsolatot vizsgálja, és segít az ismétlődő minták detektálásában.\n",
    "\n",
    "#### Véletlen zaj ($\\epsilon_t$)\n",
    "\n",
    "A véletlen zaj az idősor azon komponense, amely nem magyarázható a fent említett összetevők egyikével sem. Ez a rész véletlen fluktuációkat tartalmaz, amelyeket gyakran normális eloszlásúnak feltételezünk, az alábbi alakban:\n",
    "\n",
    "$$\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "ahol $\\sigma^2$ a zaj varianciája. A véletlen zaj figyelembevétele különösen fontos a modellépítés során, mivel a tüzött zaj \"tüllesztett\" eredményekhez, ami rontja az előrejelzések pontosságát.\n"
   ],
   "id": "ab11b740e933165b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Idősorok stacionaritása\n",
    "\n",
    "A stacionaritás az idősor statisztikai tulajdonságainak állandóságát jelenti időben. Ez az elemzési feladat ellengedhetetlen az olyan klasszikus modellek alkalmazásához, mint az ARIMA.\n",
    "\n",
    "#### Matematikai definíció\n",
    "\n",
    "Egy idősor $\\{Y_t\\}$ stacioner, ha:\n",
    "* Az idősor várható értéke konstans: $\\mathbb{E}[Y_t] = \\mu$\n",
    "* Az idősor varianciája állandó: $\\text{Var}[Y_t] = \\sigma^2$\n",
    "* Az idősor autokorrelációja csak az időeltolódástól függ: $\\text{Cov}[Y_t, Y_{t-k}] = \\gamma(k)$.\n",
    "\n",
    "Ha az idősor nem stacioner, a modellezés előtt gyakran differenciálást alkalmazunk:\n",
    "\n",
    "$$\\Delta Y_t = Y_t - Y_{t-1}$$\n",
    "\n",
    "A differenciálás során a trend eltávolítható, és az idősor stacionáriussá tehető.\n",
    "\n",
    "#### Stacionaritás tesztelése\n",
    "\n",
    "Különböző statisztikai tesztek használatosak a stacionaritás ellenőrzésére:\n",
    "\n",
    "1. Augmented Dickey-Fuller (ADF) teszt: Ez a teszt a nullhipotézisként feltételezi, hogy az idősor nem stacioner. A p-érték alapján eldönthető, hogy elutasítjuk-e a hipotézist.\n",
    "2. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) teszt: Ez a teszt a stacionaritást vizsgálja mint nullhipotézist, és ellenőrzést ad arra, hogy az idősor tartalmaz-e trendkomponenseket.\n"
   ],
   "id": "3d4f366433312dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Idősorok elemzésének modelljei\n",
    "\n",
    "#### Mozgóátlag modellek (MA)\n",
    "A mozgóátlag modellek a múltbeli hibák súlyozott átlagára alapján becsülik meg az idősor aktuális értékét. Egy $q$-rendű MA modell általános formája:\n",
    "\n",
    "$$Y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n",
    "\n",
    "ahol $\\mu$ az idősor átlaga, $\\theta_i$ pedig a múltbeli hibák súlyai.\n",
    "\n",
    "#### Autoregresszív modellek (AR)\n",
    "Az autoregresszív modellek az idősor jövőbeli értékét a múltbeli értékek lineáris kombinációjaként becsülik meg. Egy $p$-rendű AR($p$) modell matematikai formája a következőképpen írható fel:\n",
    "\n",
    "$$Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\cdots + \\phi_p Y_{t-p} + \\epsilon_t$$\n",
    "\n",
    "ahol:\n",
    "* $Y_t$ az idősor aktuális értéke,\n",
    "* $\\phi_1, \\phi_2, \\ldots, \\phi_p$ a múltbeli értékek súlyai (autoregressziós paraméterek),\n",
    "* $\\epsilon_t$ a véletlen zaj, amely $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ feltételezéssel írunk le.\n",
    "\n",
    "##### Az autoregresszív modellek tulajdonságai:\n",
    "1. Lineáris előrejelzés: Az AR($p$) modell a múltbeli értékek egyenes arányban befolyásolják a jövőt. A súlyok nagysága és előjele meghatározza, hogy az egyes múltbeli értékek mennyire erősen és milyen irányban hatnak az aktuális értékre.\n",
    "2. Rend ($p$) meghatározása: Az autoregresszív modell rendjét az adatok viselkedésének és az autokorrelációs függvény (ACF) elemzésével határozhatjuk meg. Az ACF megmutatja, hogy az idősor korábbi értékei hogyan függnek össze a korábbi értékekkel.\n",
    "3. Stacionaritási követelmény: Az AR($p$) modellek csak akkor működnek helyesen, ha az idősor stacioner. Az AR-paraméterek abszolútértékének összege egy adott feltétel szerint korlátozott:\n",
    "\n",
    "$$\\sum_{i=1}^p |\\phi_i| < 1$$\n",
    "\n",
    "##### Példa egy AR(1) modellre:\n",
    "Egy $p = 1$-rendű autoregresszív modell (AR(1)) a legegyszerűbb forma, amely a következőként írható fel:\n",
    "\n",
    "$$Y_t = \\phi_1 Y_{t-1} + \\epsilon_t$$\n",
    "\n",
    "Ebben az esetben az aktuális érték kizárólag az előző időpont értékétől függ. Ha $\\phi_1 > 0$, akkor a múltbeli értékek pozitívan korrelálnak az aktuális értékkel, míg $\\phi_1 < 0$ esetén a kapcsolat inverz.\n",
    "\n",
    "#### ARIMA modellek\n",
    "\n",
    "Az Autoregresszív Integrált Mozgóátlag (ARIMA) modellek az idősor elemzésének és előrejelzésének széles körben alkalmazott eszközei. Az ARIMA modellek az autoregressziót (AR), a differenciálást (I) és a mozgóátlagot (MA) kombinálják.\n",
    "\n",
    "##### Az ARIMA modell általános formája\n",
    "\n",
    "Az ARIMA modell matematikai egyenlete így írható fel:\n",
    "\n",
    "$$\\Phi_p(B)(1 - B)^dY_t = \\Theta_q(B)\\epsilon_t$$\n",
    "\n",
    "ahol:\n",
    "* $\\Phi_p(B) = 1 - \\phi_1 B - \\phi_2 B^2 - \\cdots - \\phi_p B^p$ az autoregresszív komponens,\n",
    "* $(1 - B)^d$ az integrálási (differenciálási) operátor a trend eltávolítására,\n",
    "* $\\Theta_q(B) = 1 + \\theta_1 B + \\theta_2 B^2 + \\cdots + \\theta_q B^q$ a mozgóátlag komponens,\n",
    "* $B$ az eltoló operátor, amelyet $Y_t = Y_{t-1}$ definícióval használunk.\n",
    "\n",
    "##### Paraméterek $(p, d, q)$ jelentése\n",
    "1. $p$ - autoregresszív (AR) komponens rendje: Az idősor aktuális értékét a múltbeli értékek száma alapján becsüli.\n",
    "2. $d$ - differenciálás mértéke: Az idősor stacionaritását biztosító differenciálások száma.\n",
    "3. $q$ - mozgóátlag (MA) komponens rendje: Az idősor aktuális értékét a múltbeli hibák alapján becsüli.\n",
    "\n",
    "##### Az ARIMA modellépítés lépései\n",
    "1. Adatok stacionaritásának vizsgálata: Augmented Dickey-Fuller (ADF) teszttel ellenőrizzük, hogy az idősor stacioner-e. Ha nem, differenciálást végzünk.\n",
    "2. Paraméterek meghatározása: Az ACF és a parciális autokorrelációs függvény (PACF) elemzésével meghatározzuk az optimális $p, d$ és $q$ értékeket.\n",
    "3. Modell illesztése: A kapott paraméterek alapján felépítjük az ARIMA modellt.\n",
    "4. Diagnosztikai vizsgálat: A modell maradékai (reziduálisokat) vizsgáljuk, hogy megfelelnek-e a fehér zaj tulajdonságainak.\n",
    "\n",
    "#### SARIMA modellek\n",
    "\n",
    "A SARIMA modellek az ARIMA kiterjesztett változatai, amelyek a szezonális mintázatokat is kezelik. Az általános modell neve: Szezonális Autoregresszív Integrált Mozgóátlag.\n",
    "\n",
    "##### SARIMA modell általános formája\n",
    "\n",
    "$$\\Phi_p(B)\\Phi_P(B^s)(1 - B)^d(1 - B^s)^DY_t = \\Theta_q(B)\\Theta_Q(B^s)\\epsilon_t$$\n",
    "\n",
    "ahol:\n",
    "* $\\Phi_P(B^s)$ és $\\Theta_Q(B^s)$ a szezonális AR és MA komponensek,\n",
    "* $D$ a szezonális differenciálás mértéke,\n",
    "* $s$ a szezonális periódus.\n",
    "\n",
    "A SARIMA modellek különösen alkalmasak az olyan idősorokra, amelyek egyaránt tartalmaznak trendeket és szezonális mintázatokat, például havi eladási adatokra.\n"
   ],
   "id": "eb1c82d67f251ef8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Természetes nyelvfeldolgozás (NLP)\n",
    "\n",
    "### NLP alapfogalmak és technikák\n",
    "A természetes nyelvfeldolgozás (Natural Language Processing, NLP) az a terület, amelynek célja az emberi nyelv számítógépes feldolgozása és megértése. Az NLP a mesterséges intelligencia feladatkörében helyezkedik el, és olyan feladatokat old meg, mint a szövegelemzés, gépi fordítás, nyelgenerálás és beszédfelismerés.\n",
    "\n",
    "#### Előfeldolgozási lépések\n",
    "Az NLP folyamatának első lépése az adat előkészítése, amely kulcsfontosságú a pontos elemzéshez és modellépítéshez. Az alábbi lépéseket tartalmazza:\n",
    "\n",
    "1. Tokenizálás:\n",
    "A szöveget kisebb egységekre, úgynevezett tokenekre bontjuk, amelyek lehetnek szavak, számok vagy egyéb jelek. Például: \"Ez\", \"egy\", \"példa\".\n",
    "A tokenizálás alapvető célja, hogy az elemzések számára strukturált adatokat biztosítson.\n",
    "\n",
    "2. Lemmatizálás és szótövezés:\n",
    "Ezek az eljárások a szavakat az alapformájukra redukálják. A lemmatizálás figyelembe veszi a szavak nyelvtani szerepét, míg a szótövezés (stemming) egyszerű egygyökű szabályokat használ.\n",
    "Például a \"futott\", \"futva\" és \"futás\" szavakat a \"fut\" alapformára hozza.\n",
    "\n",
    "3. Stop szavak eltávolítása:\n",
    "Az olyan gyakori szavak, mint \"a\", \"ez\", \"vagy\", \"is\", általában nem hordoznak értékes információt, így ezeket gyakran kiszűrik az elemzés során.\n",
    "\n",
    "4. Zajszűrés:\n",
    "A szövegből az irreleváns karakterek, jelek eltávolítják az elemzés során.\n",
    "\n",
    "#### Szövegreprezentációs módszerek\n",
    "\n",
    "Az NLP modelljei számára a nyelvi adatok számok formájában történő megjelenítése szükséges. Az alábbiakban néhány elterjed módszert ismertetünk:\n",
    "\n",
    "1. Bag of Words (BoW):\n",
    "A szöveget szavak halmazaként reprezentálja, ahol az egyes szavak előfordulásának gyakorisága rögzíti. Hátránya, hogy figyelmen kívül hagyja a szavak sorrendjét és kontextusát.\n",
    "\n",
    "2. Term Frequency-Inverse Document Frequency (TF-IDF):\n",
    "Ez a módszer a szavak relatív fontosságát méri egy dokumentumban, figyelembe véve, hogy egy adott szó milyen gyakran fordul elő az összes dokumentumban. A TF-IDF képlete:\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$$\n",
    "\n",
    "ahol:\n",
    "* $\\text{TF}(t, d)$: a $t$ szó gyakorisága a $d$ dokumentumban,\n",
    "* $\\text{IDF}(t, D) = \\log \\frac{|D|}{1 + |\\{d \\in D : t \\in d\\}|}$: az inverz dokumentumfrekvencia.\n",
    "\n",
    "3. Word embeddings (Word2Vec, GloVe):\n",
    "Ezek a módszerek a szavak sűrű vektorreprezentációját állítják elő, amely figyelembe veszi a szavak kontextusát és hasonlóságát. A Word2Vec például a szavak n-dimenziós térben történő elhelyezésével nyeri ki a szavak vektori közötti kapcsolatokat.\n",
    "\n",
    "4. Kontextuális embeddings (BERT, GPT):\n",
    "Ezek az új generációs modellek nemcsak a szavak jelentését veszik figyelembe, hanem azok környezetét is. A BERT (Bidirectional Encoder Representations from Transformers) például pontosabb kontextusba ágyazott szóvektorokat biztosít.\n",
    "\n",
    "#### Szemantikai elemzés\n",
    "\n",
    "Az NLP-ben a szemantikai elemzés az emberi nyelv jelentésének feltárását célozza. Az alábbiakban a legfontosabb alkalmazásokat ismertetjük:\n",
    "\n",
    "1. Sentiment analízis:\n",
    "Ez a technika a szöveg érzelmi tónusának (pozitív, semleges, negatív) azonosítására irányul. Alkalmazása elterjed a közösségi médiaelemzésekben és a vásárlói visszajelzések feldolgozásában.\n",
    "\n",
    "2. Névelem-felismerés (NER):\n",
    "A szövegben szereplő entitások (személyek, helyek, dátumok) automatikus felismerése és osztályozása. Például egy \"Google\" entitást vállalatként azonosítja.\n",
    "\n",
    "3. Témamodellezés:\n",
    "Az LDA (Latent Dirichlet Allocation) segítségével a szövegből különböző témák automatikus kinyerése valósítható meg. Az LDA feltételezi, hogy a dokumentumok kevert témákat tartalmaznak, és minden téma adott szavak halmazát reprezentálja.\n",
    "\n",
    "### Modern NLP architektúrák\n",
    "#### Transformer architektúra\n",
    "A Transformer modell a természetes nyelvfeldolgozás forradalmiszemközének, amely figyfyelmi (self-attention) alapú. Az architektúra alapegyenlete:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}),V$$\n",
    "\n",
    "ahol:\n",
    "* Q: a lekérdezési mátrix (queries),\n",
    "* K: a kulcsok mátrixa (keys),\n",
    "* V: az értékek mátrixa (values),\n",
    "* $d_k$: a kulcsmátrix dimenziója.\n",
    "\n",
    "#### Előtanított nyelvi modellek\n",
    "A BERT (Bidirectional Encoder Representations from Transformers) az evolúziót, mint például a FinBERT - jelentős szerepet játszanak a pénzügyi szövegek feldolgozásában. A FinBERT + kifejezetten pénzügyi adatokra finomhangolható, így hatékonyabb szentiméntelemzésre és piacokkal kapcsolatos szövegek feldolgozására.\n"
   ],
   "id": "9e6184eb80b651e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pénzügyi piacok elemzése\n",
    "### Piaci hatékonyság és előrejelezhetőség\n",
    "A pénzügyi piacok hatékonysága és előrejelezhetősége kulcsfontosságú kérdés a modern közgazdaságtanban és pénzügyi elemzésben. Az Efficient Market Hypothesis (EMH), vagyis a hatékony piacok elmélete, Eugene Fama nevéhez kötődik, aki 1970-ben alkotta meg az elmélet alapjait. Az elmélet szerint a piacok olyan hatékonyak, hogy minden releváns információ azonnal és teljes mértékben tükröződik az eszközök árában.\n",
    "\n",
    "#### Piaci hatékonyság típusai\n",
    "Az EMH három fő formát különböztet meg:\n",
    "\n",
    "1. Gyenge forma:\n",
    "   Ez azt állítja, hogy az árfolyamok csak a múltbeli adatokat és volumeneiket tükrözik. Ennek következménye, hogy a technikai elemzés nem nyújt előnyt az árba. Matematikailag szempontból a piaci árfolyamokat véletlenszerű (randomwalk) modellezzük:\n",
    "\n",
    "   $$P_t = P_{t-1} + \\epsilon_t$$\n",
    "\n",
    "   ahol:\n",
    "   * $P_t$: az árfolyam t időpontban,\n",
    "   * $\\epsilon_t$: független és azonos eloszlású véletlen zaj.\n",
    "\n",
    "2. Közepes forma:\n",
    "   Ez azt állítja, hogy az árfolyamok nemcsak a múltbeli adatokat, hanem az összes nyilvánosan elérhető információt is tartalmazzák. Ebben beletartoznak a vállalati jelentések, gazdasági hírek és politikai események. Ez az állítás megkérdőjelezi a fundamentális elemzés hatékonyságát.\n",
    "\n",
    "3. Erős forma:\n",
    "   Ez a forma állítja, hogy az árák minden információt tükröznek, beleértve a nyilvános és a bennfentes információkat is. Ha ez igaz lenne, senki sem tudna tartósan felülteljesíteni a piacot, még a bennfentes információk felhasználásával sem.\n",
    "\n",
    "#### Piaci anomáliák\n",
    "Bár az EMH széles körben elfogadott, számos kutatás mutatott ki anomáliákat, amelyek az elmélet gyengeségeire utalnak:\n",
    "\n",
    "* Hét napja hatás: Bizonyos napokon (például pénteken) az árfolyamok szisztematikusan eltérnek az átlagtól.\n",
    "* Január hatás: Az év első hónapjában gyakran magasabb hozamok figyelhetők meg.\n",
    "* Bennfentes kereskedelem: A bennfentes információk használata előnyt jelenthet, különösen az erős formájú piacok esetében.\n",
    "\n",
    "#### Piaci szentiméntés és viselkedési pénzügyek\n",
    "A piaci szentiméntés, vagyis a befektetők általános érzelmi állapota, jelentős hatással van az árfolyamokra. A viselkedési pénzügyek (behavioral finance) ezen hatásokat próbálja megérteni és modellezni.\n",
    "\n",
    "#### Befektetői hangulat mérése\n",
    "1. Közvetlen mérési módszerek:\n",
    "   A befektetők körében végzett felmérések és kérdőívek segítségével közvetlenül mérhetjük a piaci szentiméntet. Például az AAII Investor Sentiment Survey az amerikai befektetők optimizmusát és semlegességét méri.\n",
    "\n",
    "2. Közvetett indikátorok:\n",
    "   A piaci mutatók, például a VIX index (volatility index), a piaci kockázatvállalást és szentiméntet közvetett módon mérésére szolgálnak. Magas VIX-értékek jellemzően félelmet jeleznek a piacokon.\n",
    "\n",
    "3. Média alapú szentiméntés:\n",
    "   A hírek, közösségi média bejegyzések és pénzügyi elemzések automatikus szövegelemzése segítségével a piaci hangulat kvantitatív módon mérhető. Az NLP és szentiméntanalízis, például a FinBERT modell segítségével, fontos szerepet játszanak e folyamatban.\n",
    "\n",
    "#### Viselkedési torzítások\n",
    "A befektetők döntéseit gyakran irracionalitási tényezők befolyásolják, amelyek torzítják a piacok működését:\n",
    "\n",
    "1. Nyájhatás (herding effect):\n",
    "   A befektetők hajlamosak mások viselkedését követni, különösen bizonytalan környezetben. Ez jelentős árfolyam-ingadozást okozhat.\n",
    "\n",
    "2. Túlzott magabiztosság (overconfidence):\n",
    "   A befektetők túlbecsülik saját tudásukat és képességeiket, ami gyakran túlzott kockázatvállaláshoz vezet.\n",
    "\n",
    "3. Lehorgonyzás (anchoring):\n",
    "   Az árfolyamok elemzése során az emberek gyakran túl nagy jelentőséget tulajdonítanak egy korábbi értéknek vagy eseménynek, ami befolyásolja döntéseiket.\n"
   ],
   "id": "f03cc8ae52efaacd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hibrid modellezési megközelítések\n",
    "### Idősoros és szöveges adatok integrálása\n",
    "A különböző adatforrások egyesítése a pénzügyi modellezés új dimenzióját nyitja meg. Az idősorelemzés és az NLP együttes alkalmazása a strukturált (árfolyamok) és strukturálatlan (szövegek) adatok integrálására.\n",
    "\n",
    "#### Feature engineering\n",
    "Az adatokból kinyert jellemzők (features) alapvető szerepet játszanak az integrált modellek teljesítményében:\n",
    "\n",
    "1. Idősor jellemzők:\n",
    "   Az idősorokból statisztikai mutatókat nyerhetünk ki, mint például a mozgóátlagok, volatilitás, maximumok és minimumok.\n",
    "\n",
    "2. Szöveges jellemzők:\n",
    "   A szentiméntanalízis eredményeit, a TF-IDF értékeket vagy a szóbeágyazásokat kombinálhatjuk az idősorokkal. Például egy hír szentiméntjét és a hozzá kapcsolódó árfolyamváltozást egyaránt modellezhetjük.\n",
    "\n",
    "3. Jellemzők kombinálása:\n",
    "   Az idősor- és szöveges jellemzők egyesítése lehetővé teszi a komplex összefüggések feltárását.\n",
    "\n",
    "#### Modell architektúrák\n",
    "Az integrált adatelemzés különböző modellarchitektúrákat igényel:\n",
    "\n",
    "1. Párhuzamos feldolgozás:\n",
    "   Az idősor- és szöveges adatokat külön modellekben dolgozzák fel, majd az eredményeket kombinálják.\n",
    "\n",
    "2. Kaszkád modellek:\n",
    "   Az egyik modell eredményeit felhasználják egy másik modell számára.\n",
    "\n",
    "3. Ensemble módszerek:\n",
    "   Több modell eredményeinek kombinálásával növelhető az előrejelzések pontossága.\n",
    "\n",
    "### Mély tanulási alapú megoldások\n",
    "\n",
    "A mély tanulási (Deep Learning) módszerek az utóbbi években forradalmasították a pénzügyi előrejelzést és a természetes nyelvfeldolgozást. Az idősorok és szöveges adatok integrációjában különösen fontos szerepet játszanak az olyan modellek, amelyek képesek a komplex, nemlineáris kapcsolatok azonosítására.\n",
    "\n",
    "#### Rekurrens neurális hálózatok (RNN)\n",
    "\n",
    "Az RNN-ek alkalmasak az idősorok adatok modellezésére, mivel képesek kezelni a függőségeket képesek kezelni. Az alap RNN-ek azonban hosszú távú függőségekkel nem tudnak hatékonyan megbirkózni, ezért fejlesztették ki a következő speciális változatokat:\n",
    "\n",
    "1. LSTM (Long Short-Term Memory):\n",
    "   Az LSTM egységek képesek a hosszú távú függőségek kezelésére, mivel a memóriakap szabályozzák, hogy az információ hogyan áramlik a hálózaton keresztül:\n",
    "\n",
    "   $$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f),\\quad i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i),$$\n",
    "\n",
    "   ahol:\n",
    "   * $f_t$: felejtési kapu,\n",
    "   * $i_t$: beviteli kapu,\n",
    "   * $\\sigma$: aktivációs függvény,\n",
    "   * $W$ és $b$: súlyok és bias paraméterek.\n",
    "\n",
    "2. GRU (Gated Recurrent Unit):\n",
    "   A GRU egy egyszerűbb alternatíva az LSTM-hez, amely kevesebb paramétert használ. Gyakrabban tanulást biztosít.\n",
    "\n",
    "#### Konvolúciós neurális hálózatok (CNN)\n",
    "\n",
    "A CNN-ek hagyományosan képfeldolgozásra lettek kifejlesztve, de időbeli mintázatok felismerésére is alkalmazhatók az idősor-elemzés során. A konvolúciós rétegek képesek a rövid távú mintázatok (például időben változások) azonosítására:\n",
    "\n",
    "$$z_{i,j} = \\sigma\\left(\\sum_{m,n} x_{i+m,j+n} \\cdot w_{m,n} + b\\right),$$\n",
    "\n",
    "ahol:\n",
    "* $x$: bementi adatok,\n",
    "* $w$: konvolúciós súlyok,\n",
    "* $b$: bias,\n",
    "* $\\sigma$: aktivációs függvény.\n",
    "\n",
    "#### Hibrid modellek\n",
    "\n",
    "A CNN és RNN kombinációja lehetővé teszi az idősorok és szöveges adatok egységes feldolgozását. A CNN-ek az időbeli mintázatokat, míg az LSTM vagy GRU az időbeli függőségeket azonosítják.\n",
    "\n",
    "1. CNN-LSTM:\n",
    "   A CNN-ek az idősorosadatok jellemzőinek kinyerésére, az LSTM pedig a hosszú távú kapcsolatok felismerésére használhatók. Ez különösen hasznos, ha az idősoradat-jellemzőket az árfolyam-előrejelzéshez kombinálják.\n",
    "\n",
    "2. Attention mechanizmus:\n",
    "   Az attention mechanizmus lehetővé teszi a modell számára, hogy az adatok releváns részeire fókuszáljon. Ez különösen hasznos, ha egy szöveg bizonyos része vagy egy időbeli árfolyammérték nagyobb jelentőséggel bír:\n",
    "\n",
    "   $$Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,$$\n",
    "\n",
    "   ahol:\n",
    "   * Q: lekérdezés (query) mátrix,\n",
    "   * K: kulcs (key) mátrix,\n",
    "   * V: érték (value) mátrix,\n",
    "   * $d_k$: dimenzióméret.\n",
    "\n",
    "3. Transformer alapú megoldások:\n",
    "   A Transformer architektúra önmagában is használható az idősorok és szöveges adatok elemzésére. A self-attention mechanizmus révén hatékonyan dolgozza fel az adatokat, és képes a hosszú távú kapcsolatok azonosítására is.\n",
    "\n",
    "## Teljesítményértékelés és validáció\n",
    "\n",
    "### Előrejelzési metrikák\n",
    "A modellek értékelésére többféle mutató is alkalmazható, amelyek lehetővé teszik a pontosság és a gyakorlati használhatóság megítélését.\n",
    "\n",
    "#### Statisztikai metrikák\n",
    "\n",
    "1. Mean Absolute Error (MAE):\n",
    "   Az abszolút hibák átlaga, amely a modellek általános pontosságát méri:\n",
    "\n",
    "   $$MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|,$$\n",
    "\n",
    "   ahol:\n",
    "   * $y_i$: valódi érték,\n",
    "   * $\\hat{y}_i$: előrejelzett érték.\n",
    "\n",
    "2. Root Mean Square Error (RMSE):\n",
    "   Az RMSE nagyobb súlyt ad a nagyobb hibáknak, ezért jobbán kiemeli a modell nagyobb tévedéseit.\n",
    "\n",
    "   $$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}.$$\n",
    "\n",
    "3. Mean Absolute Percentage Error (MAPE):\n",
    "   Az előrejelzési hibák relatív arányát méri:\n",
    "\n",
    "   $$MAPE = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\cdot 100.$$\n",
    "\n",
    "#### Pénzügyi metrikák\n",
    "\n",
    "1. Sharpe-ráta:\n",
    "   A hozam és kockázat arányát méri:\n",
    "\n",
    "   $$S = \\frac{\\bar{R} - R_f}{\\sigma},$$\n",
    "\n",
    "   ahol:\n",
    "   * $\\bar{R}$: portfólió átlaghozama,\n",
    "   * $R_f$: kockázatmentes hozam,\n",
    "   * $\\sigma$: portfólió szórása.\n",
    "\n",
    "2. Maximum drawdown:\n",
    "   A legnagyobb csökkenés egy csúcsról egy mélypontra:\n",
    "\n",
    "   $$MDD = \\frac{\\text{Peak Value} - \\text{Trough Value}}{\\text{Peak Value}}.$$\n",
    "\n",
    "3. Profit és veszteség (P&L):\n",
    "   Az előrejelzések alapján végrehajtott stratégiák nyereségességét méri.\n",
    "\n",
    "#### Validációs stratégiák\n",
    "A model robusztusságának és megbízhatóságának tesztelése kritikus fontosságú:\n",
    "\n",
    "1. Időbeli keresztvalidáció:\n",
    "   Az idősorosadatok időbeli függősége miatt hagyományos keresztvalidáció nem használható.\n",
    "   Az időbeli keresztvalidáció lépései:\n",
    "   * A múltbeli adatokból tanul a modell.\n",
    "   * A jövőbeli adatokon teszteli magát.\n",
    "\n",
    "2. Backtesting:\n",
    "   A történeti szimulációk révén a modell előrejelzését valós pénzi adatok alapján tesztelik.\n",
    "   Fontos figyelembe venni a tranzakciós költségeket és a piaci körülményeket.\n"
   ],
   "id": "375b1171f19915fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fejlesztői Dokumentáció és Implementáció",
   "id": "197ba630aeb1df5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Python nyelv és eszközeinek áttekintése\n",
    "\n",
    "A Python az egyik legszélesebb körben használt programozási nyelv a mesterséges intelligencia, gépi tanulás, adatelemzés és pénzügyi modellezés területén. A nyelv egyszerű szintaxisa, széles körű könyvtárainak ökoszisztémája és interaktív eszközei miatt ideális választás a szakdolgozathoz kapcsolódó idősor-elemzés és természetes nyelvfeldolgozási (NLP) feladatokhoz.\n",
    "\n",
    "A Python erőssége az eszközök egyszerű integrációjában rejlik. Az idősor-elemzés és NLP kombinációja érdekében az adatok előkészítésétől kezdve a modellezésen át az értékelésig lehetőség van egyetlen keretrendszerben lehet dolgozni.\n",
    "\n",
    "    1. Adatintegráció és előfeldolgozás:\n",
    "    A Pandas és a NumPy használható az idősorok előkészítésére, míg a szöveges adatok tisztításához az NLTK alkalmazható.\n",
    "\n",
    "    2. Modellezés:\n",
    "    A Scikit-learn és a TensorFlow eszközei lehetővé teszik a klasszikus és mélytanulási modellek egyidejű fejlesztését.\n",
    "\n",
    "    4. Validáció és értékelés:\n",
    "    Az idősoros validációhoz a Scikit-learn, míg a pénzügyi metrikák kiszámításához a NumPy és Pandas eszközök használhatók."
   ],
   "id": "24e374f101c4eed0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Használt könyvtárak",
   "id": "fe2630c28a3743d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ast\n",
    "\n",
    "Az ast modul a Python beépített könyvtára, amely az absztrakt szintaxisfák (AST) kezelését támogatja.\n"
   ],
   "id": "63721075e85a791c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import ast",
   "id": "bf80e9cd5f169e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### numpy\n",
    "\n",
    "A NumPy a Python egyik legfontosabb könyvtára a numerikus számításokhoz. Széleskörű támogatást nyújt tömbműveletekhez, lineáris algebrához és statisztikai számításokhoz."
   ],
   "id": "562ca8e87dd63799"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np",
   "id": "af37f10124e4246a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### pandas\n",
    "\n",
    "A Pandas könyvtár adatok kezelésére és elemzésére szolgál. A DataFrame struktúrája ideális táblázatos adatok manipulációjára."
   ],
   "id": "5af17ec390b87de5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import pandas as pd",
   "id": "6358d6e6f77a3dcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### transformers\n",
    "\n",
    "Transformers könyvtár\n",
    "\n",
    "A Hugging Face Transformers könyvtár a modern NLP-modellek, például a BERT vagy GPT alkalmazását teszi lehetővé.\n",
    "\n",
    "#### Modulok bemutatása:\n",
    "* AutoTokenizer: Szövegek tokenizálásához használható."
   ],
   "id": "b4f0fa4b1f0ba9ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from transformers import AutoTokenizer",
   "id": "a53cc55a1a8d260e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* AutoModelForSequenceClassification: Előtanított modellek betöltése osztályozási feladatokhoz.",
   "id": "973bb7300a91c355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from transformers import AutoModelForSequenceClassification",
   "id": "a75b102e5f415127"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* pipeline: Egyszerűsített interfész különböző NLP-feladatokhoz, például szentimentelemzéshez.",
   "id": "62b1be6f57327453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from transformers import pipeline",
   "id": "b49e123f94a5b0b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### matplotlib és plotly",
   "id": "72e58b0a714712a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A Matplotlib az egyik legszélesebb körben használt adatvizualizációs könyvtár.",
   "id": "8baced74ac06c54b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "plt.style.use('ggplot')"
   ],
   "id": "a30793b03d8bb2af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A Plotly interaktív adatvizualizációs könyvtár.",
   "id": "fee03ee69a7dc565"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import plotly.graph_objects as go",
   "id": "d6b68f2e049af3d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### yfinance\n",
    "\n",
    "A yfinance könyvtár egyszerű és gyors hozzáférést biztosít a Yahoo Finance API-jához, ami elérhetővé teszi a tőzsdei adatok lekérdezését."
   ],
   "id": "c3cd5f8296d82a21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import yfinance as yf",
   "id": "d9ae6eabcd3e41d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### backtrader\n",
    "\n",
    "A backtrader egy tőzsdei kereskedési stratégiák szimulálására alkalmas könyvtár."
   ],
   "id": "73093f659f2814b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import backtrader as bt",
   "id": "29b3ae92f89e923b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### tqdm\n",
    "\n",
    "A tqdm egy egyszerű, de hatékony eszköz a Python programok futási állapotának vizualizálására."
   ],
   "id": "11637fe84f24b896"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from tqdm.notebook import tqdm",
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## NLP modellek\n",
    "\n",
    "A Huggingface honlapján találkható előre betanított modellek lekérése és felhasználása egy transformers pipeline létrehozásához."
   ],
   "id": "32573c757fff0969"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DistilRoBERTa\n",
    "\n",
    "Ez a modell a RoBERTa-bázismodell desztillált változata a Financial PhraseBank adat gyűjteményén finomhangolva.\n",
    "A modell kis- és nagybetű-érzékeny, továbbá 6 réteggel, 768 dimenzióval és 12 fejjel rendelkezik, összesen 82M paraméterrel.\n",
    "\n",
    "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
   ],
   "id": "d662b446a895b334"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_DistRoBERTa = f\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
    "\n",
    "\n",
    "_sentiment_analysis_DistRoBERTa = pipeline(\"sentiment-analysis\",\n",
    "                                           model= AutoModelForSequenceClassification.from_pretrained(\n",
    "                                               model_DistRoBERTa,\n",
    "                                               num_labels=3),\n",
    "                                           tokenizer=AutoTokenizer.from_pretrained(model_DistRoBERTa),\n",
    "                                           top_k=None, padding=True, truncation=True\n",
    "                                           )"
   ],
   "id": "6eb0240a229e1ea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### FinBERT\n",
    "\n",
    "A FinBERT egy előre betanított NLP modell (BERT nyelvi modell továbbképzése) pénzügyi szövegek hangulatának elemzésére.\n",
    "Ez a modell is a Financial PhraseBank adat gyűjteményen lett finomhangolva.\n",
    "\n",
    "https://huggingface.co/ProsusAI/finbert"
   ],
   "id": "8843e8e9d3793639"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_FinBERT = \"ProsusAI/finbert\"\n",
    "_sentiment_analysis_FinBERT = pipeline(\"sentiment-analysis\",\n",
    "                                       model=AutoModelForSequenceClassification.from_pretrained(\n",
    "                                           model_FinBERT,\n",
    "                                           num_labels=3\n",
    "                                       ),\n",
    "                                       tokenizer=AutoTokenizer.from_pretrained(model_FinBERT),\n",
    "                                       top_k=None, padding=True, truncation=True\n",
    "                                       )"
   ],
   "id": "f4270dc9c2d1ae63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### DeBERTa\n",
    "\n",
    "A DeBERTa a BERT és a RoBERTa modelleket javítja a szétválasztott figyelem és a továbbfejlesztett maszkdekóder használatával.\n",
    "Ezzel a két fejlesztéssel a DeBERTa 80 GB képzési adatmennyiséggel az NLU-feladatok többségében felülmúlja a RoBERTa-t.\n",
    "\n",
    "https://huggingface.co/mrm8488/deberta-v3-ft-financial-news-sentiment-analysis"
   ],
   "id": "a9d3787517fa662b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_DeBERTa = f\"mrm8488/deberta-v3-ft-financial-news-sentiment-analysis\"\n",
    "_sentiment_analysis_DeBERTa = pipeline(\"sentiment-analysis\",\n",
    "                                       model=AutoModelForSequenceClassification.from_pretrained(\n",
    "                                           model_DeBERTa,\n",
    "                                           num_labels=3),\n",
    "                                       tokenizer=AutoTokenizer.from_pretrained(model_DeBERTa),\n",
    "                                       top_k=None, padding=True)"
   ],
   "id": "c8b888a477e13cf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Használt adatok",
   "id": "dc53944c5b24e7da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Függvények",
   "id": "2323d64e179bdb54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adatok előkészítése",
   "id": "14c47658f7a46149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_data(input_file, ticker=\"\"):\n",
    "    \"\"\"\n",
    "    Process stock data by combining news data with Yahoo Finance price data.\n",
    "    Missing stock data is filled using linear interpolation: i/(n+1) where n is the number\n",
    "    of missing days and i is the current position in the gap.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file: Path to the input CSV file containing news data\n",
    "    - ticker: Ticker symbol of the stock\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        data = pd.read_csv(input_file)\n",
    "\n",
    "        # Remove unnamed columns\n",
    "        data.drop(data.columns[data.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "        # Filter data for specific ticker\n",
    "        data = data[data['stock'] == ticker].copy()\n",
    "\n",
    "        # Convert date to datetime with UTC timezone\n",
    "        data['date'] = pd.to_datetime(data['date'], utc=True)\n",
    "\n",
    "        # Convert UTC time to local time and extract date\n",
    "        local_dates = data['date'].dt.tz_localize(None).dt.date\n",
    "\n",
    "        # Create a datetime index for the news data\n",
    "        news_data = data.groupby(local_dates)['title'].apply(lambda x: ' | '.join(x.dropna())).reset_index()\n",
    "        news_data.rename(columns={'title': 'News', 'date': 'Date'}, inplace=True)\n",
    "\n",
    "        # Convert Date column to datetime without timezone\n",
    "        news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
    "\n",
    "        # Get date range for stock data (using localized dates)\n",
    "        start_date = local_dates.min()\n",
    "        end_date = local_dates.max()\n",
    "\n",
    "        # Fetch stock data from Yahoo Finance\n",
    "        stock = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        stock = stock.reset_index()\n",
    "        stock['Date'] = pd.to_datetime(stock['Date'])  # Yahoo data comes without timezone\n",
    "\n",
    "        # Flatten multi-level columns if they exist\n",
    "        stock.columns = [col[0] if isinstance(col, tuple) else col for col in stock.columns]\n",
    "\n",
    "        # Create a complete date range including weekends\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        complete_dates = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "        # Merge stock data with complete date range\n",
    "        stock_full = pd.merge(complete_dates, stock, on='Date', how='left')\n",
    "\n",
    "        def interpolate_gaps(series):\n",
    "            \"\"\"\n",
    "            Custom interpolation function that uses i/(n+1) formula for gaps\n",
    "            \"\"\"\n",
    "            result = series.copy()\n",
    "            mask = series.isna()\n",
    "\n",
    "            if not mask.any():  # No NaNs to fill\n",
    "                return result\n",
    "\n",
    "            # Find consecutive NaN sequences\n",
    "            gaps = mask.ne(mask.shift()).cumsum()[mask]\n",
    "\n",
    "            for gap_idx in gaps.unique():\n",
    "                gap_mask = gaps == gap_idx\n",
    "                gap_size = gap_mask.sum()\n",
    "\n",
    "                # Find values before and after gap\n",
    "                before_idx = series.index[series.index < gaps[gap_mask].index[0]][-1] if any(series.index < gaps[gap_mask].index[0]) else None\n",
    "                after_idx = series.index[series.index > gaps[gap_mask].index[-1]][0] if any(series.index > gaps[gap_mask].index[-1]) else None\n",
    "\n",
    "                if before_idx is not None and after_idx is not None:\n",
    "                    # Regular gap with values on both sides\n",
    "                    start_val = series[before_idx]\n",
    "                    end_val = series[after_idx]\n",
    "                    gap_values = [start_val + (end_val - start_val) * (i / (gap_size + 1))\n",
    "                                for i in range(1, gap_size + 1)]\n",
    "                    result.iloc[gaps[gap_mask].index] = gap_values\n",
    "                elif before_idx is not None:\n",
    "                    # Gap at the end - use last known value\n",
    "                    result.iloc[gaps[gap_mask].index] = series[before_idx]\n",
    "                elif after_idx is not None:\n",
    "                    # Gap at the start - use first known value\n",
    "                    result.iloc[gaps[gap_mask].index] = series[after_idx]\n",
    "\n",
    "            return result\n",
    "\n",
    "        # Apply custom interpolation to each numeric column\n",
    "        numeric_columns = ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
    "        for col in numeric_columns:\n",
    "            stock_full[col] = interpolate_gaps(stock_full[col])\n",
    "\n",
    "        # Merge with news data\n",
    "        final_data = pd.merge(stock_full, news_data, on='Date', how='left')\n",
    "\n",
    "        # Sort by date\n",
    "        final_data = final_data.sort_values('Date')\n",
    "\n",
    "        # Convert Date back to date (not datetime) for final output\n",
    "        final_data['Date'] = final_data['Date'].dt.date\n",
    "\n",
    "        # Save the processed data\n",
    "        output_file = f\"/tmp/pycharm_project_520/src/data/input/processed/source/{ticker}.csv\"\n",
    "        final_data.to_csv(output_file, index=False)\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file}' was not found.\")\n",
    "        return None"
   ],
   "id": "bec85f1e6627bb1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sentiment értékek",
   "id": "f1f0706e91bddbba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_sentiment_scores_test(df, news, model):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis using a specified pipeline and calculates sentiment scores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the text data.\n",
    "        news (str): Column name for the news headlines or text to analyze.\n",
    "        model (str): The model to use for sentiment analysis ('DistRoBERTa', 'FinBERT', 'DeBERTa').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with calculated sentiment scores.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    # Select the appropriate model pipeline\n",
    "    if model == \"DistRoBERTa\":\n",
    "        sent_pipeline = _sentiment_analysis_DistRoBERTa\n",
    "    elif model == \"FinBERT\":\n",
    "        sent_pipeline = _sentiment_analysis_FinBERT\n",
    "    elif model == \"DeBERTa\":\n",
    "        sent_pipeline = _sentiment_analysis_DeBERTa\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose 'DistRoBERTa', 'FinBERT', 'DeBERTa'.\")\n",
    "\n",
    "    res = {}\n",
    "    fail = {}\n",
    "    n = 0\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), disable=True):\n",
    "        text = row[news]\n",
    "        try:\n",
    "            if pd.isna(text):\n",
    "                result = [[\n",
    "                    {'label': 'neutral', 'score': 1.0},\n",
    "                    {'label': 'negative', 'score': 0},\n",
    "                    {'label': 'positive', 'score': 0}\n",
    "                ]]\n",
    "            else:\n",
    "                result = sent_pipeline(text)\n",
    "\n",
    "        except (IndexError, RuntimeError):\n",
    "            if pd.isna(text):\n",
    "                fail[n] = str(i)\n",
    "            else:\n",
    "                fail[n] = text\n",
    "                n += 1\n",
    "            result = [[\n",
    "                {'label': 'neutral', 'score': 1.0},\n",
    "                {'label': 'negative', 'score': 0},\n",
    "                {'label': 'positive', 'score': 0}\n",
    "            ]]\n",
    "            pass\n",
    "        res[i] = result[0]  # Extract the first list\n",
    "\n",
    "    # Create a new column with the sentiment score\n",
    "    sentiment_score_col = f\"{model}_sentiment_scores\"\n",
    "    df[sentiment_score_col] = res\n",
    "\n",
    "    return df"
   ],
   "id": "d576edd10d71f89e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sentiment_vector(df, scores, model):\n",
    "    \"\"\"\n",
    "    Creates a sentiment vector column based on sentiment scores.\n",
    "    (Neutral, Positive, Negative)\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the stock data.\n",
    "    - scores: Column name for the sentiment scores.\n",
    "    - model: Name of the model.\n",
    "    - debug: If True, prints debugging information.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Function to extract scores and create a sentiment vector\n",
    "    def extract_scores(row):\n",
    "        # If row is a string, convert it to a Python object\n",
    "        if isinstance(row, str):\n",
    "            row = ast.literal_eval(row)  # Safely evaluate the string to a Python object\n",
    "        # Extract scores for neutral, positive, and negative in the correct order\n",
    "        dimensions = {item['label']: item['score'] for item in row}\n",
    "        return dimensions['neutral'], dimensions['positive'], dimensions['negative']\n",
    "\n",
    "    # Apply the function to the column\n",
    "    sentiment_vector_col = f\"{model}_sentiment_vector\"\n",
    "    df[sentiment_vector_col] = df[scores].apply(extract_scores)\n",
    "\n",
    "    return df"
   ],
   "id": "9dd9dc9826dae89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def adjust_scores(df, vectors, model):\n",
    "    \"\"\"Calculate adjusted sentiment scores from vector column.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    def calculate_normalized_score(vector):\n",
    "        neutral, positive, negative = vector\n",
    "        total = positive + negative #+ neutral\n",
    "        return (positive - negative) / total if total > 0 else 0\n",
    "\n",
    "    def calculate_raw_score(vector):\n",
    "        neutral, positive, negative = vector\n",
    "        return positive if positive > negative else (-1*negative) if negative > positive else 0\n",
    "\n",
    "\n",
    "    df[f\"{model}_adjusted_score\"] = df[vectors].apply(calculate_normalized_score)\n",
    "    df[f\"{model}_raw_score\"] = df[vectors].apply(calculate_raw_score)\n",
    "\n",
    "    return df"
   ],
   "id": "4d3578745d2a6986",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Kapott és jósolt adatok összehasonlítása",
   "id": "b570f3dcd9cfaed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_trend(df, close, day=1):\n",
    "    \"\"\"\n",
    "    Calculates the trend of a stock price.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the stock data.\n",
    "    - close: Column name for the close price.\n",
    "    - day: Number of days to calculate the trend.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate the trend\n",
    "        tend_col = f\"trend_{day}d\"\n",
    "        df[tend_col] = df[close].pct_change(periods=day).shift(-1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {str(e)}\")\n",
    "        return df"
   ],
   "id": "8e909a83a3aab862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_volatility(df, close, window=5, trading_days=252):\n",
    "    \"\"\"\n",
    "    Calculate rolling volatility for a DataFrame's close price column.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame containing price data\n",
    "    close : str\n",
    "        Name of the column containing closing prices\n",
    "    window : int, optional\n",
    "        Rolling window size for volatility calculation (default: 5)\n",
    "    trading_days : int, optional\n",
    "        Annual trading days for annualized volatility (default: 252)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Original DataFrame with added volatility columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result = df.copy()\n",
    "\n",
    "    # Calculate log returns\n",
    "    result['log_returns'] = np.log(result[close] / result[close].shift(1))\n",
    "\n",
    "    # Calculate rolling standard deviation of log returns (annualized)\n",
    "    result[f'volatility_{window}d'] = (\n",
    "        result['log_returns']\n",
    "        .rolling(window=window)\n",
    "        .std()\n",
    "        * np.sqrt(trading_days)\n",
    "    )\n",
    "\n",
    "    return result"
   ],
   "id": "14391b180fd4753c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_predictions(df, tar, sco, vol, model, score_type, target_factor=0.15, score_factor = 0.5):\n",
    "    \"\"\"\n",
    "    Validate prediction based on adaptive volatility-based threshold for neutrality.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing stock data with calculated volatility.\n",
    "        tar (str): Column name for the target price change (e.g., 'Daily Return').\n",
    "        sco (str): Column name for the sentiment score.\n",
    "        vol (str): Column name for the volatility.\n",
    "        model (str): Model name to include in the predictions column.\n",
    "        score_type (str): Type of sentiment score (e.g., 'adjusted', 'raw').\n",
    "        target_factor (float): Multiplier for volatility to define the neutral threshold for the target.\n",
    "        score_factor (float): Threshold for neutral sentiment score.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with predictions and success/failure of predictions.\n",
    "    \"\"\"\n",
    "    predict_dict = {}\n",
    "\n",
    "    # Validate input columns exist\n",
    "    required_cols = [tar, sco, vol]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Calculate adaptive volatility threshold and predictions\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), disable=True):\n",
    "        try:\n",
    "            target = row[tar]\n",
    "            score = row[sco]\n",
    "\n",
    "            # Handle NaN values\n",
    "            if pd.isna(target) or pd.isna(score) or pd.isna(row[vol]):\n",
    "                predict_dict[i] = 0  # or another default value for NaN cases\n",
    "                continue\n",
    "\n",
    "\n",
    "            neu_target_threshold = abs(row[vol] * target_factor) if abs(row[vol] * target_factor) <= 1 else 1 # Add abs() for safety\n",
    "\n",
    "            #neu_score_threshold = abs(row[vol] * score_factor) if abs(row[vol] * score_factor) <= 1 else 1 # Add abs() for safety\n",
    "            neu_score_threshold = score_factor\n",
    "\n",
    "            # Improved prediction logic\n",
    "            if abs(target) > neu_target_threshold and abs(score) > neu_score_threshold:\n",
    "                # Only evaluate directional accuracy when both target and score are non-zero\n",
    "                if target != 0 and score != 0:\n",
    "                    predict_dict[i] = 1 if (target * score) > 0 else 0  # Simplified comparison\n",
    "                else:\n",
    "                    predict_dict[i] = 0  # Case where either value is zero\n",
    "            else:\n",
    "                # Modified neutral case logic\n",
    "                predict_dict[i] = 1 if abs(score) < neu_score_threshold and abs(target) < neu_target_threshold else 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"KeyError at index {i}: {str(e)}\")\n",
    "            predict_dict[i] = 0\n",
    "\n",
    "    # Create the predictions DataFrame more efficiently\n",
    "    predictions = pd.Series(predict_dict, name=f\"{model}_{score_type}_predictions\")\n",
    "\n",
    "    # Update the DataFrame more efficiently\n",
    "    if predictions.name in df.columns:\n",
    "        df = df.drop(columns=[predictions.name])\n",
    "    df = df.join(predictions)\n",
    "\n",
    "    return df"
   ],
   "id": "4d8b60e5c1e296df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adatok feldolgozása",
   "id": "fa930eac649f7916"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_data(input_file, ticker=\"\", models=('DistRoBERTa', 'FinBERT', 'DeBERTa')):\n",
    "    \"\"\"\n",
    "    Process stock data\n",
    "\n",
    "    Parameters:\n",
    "    - input_file: Path to the input CSV file containing news data\n",
    "    - ticker: Ticker symbol of the stock\n",
    "    - models: Tuple of model names to process.\n",
    "    \"\"\"\n",
    "    preprocess_data(input_file, ticker=ticker)\n",
    "\n",
    "    data = pd.read_csv(f'/tmp/pycharm_project_520/src/data/input/processed/source/{ticker}.csv')\n",
    "\n",
    "    data = add_trend(data, \"Close\")\n",
    "    data = add_volatility(data, 'Close')\n",
    "    for model in models:\n",
    "        data = get_sentiment_scores_test(data, 'News', model=model)\n",
    "        data = sentiment_vector(data, f'{model}_sentiment_scores', model)\n",
    "        data = adjust_scores(data, f'{model}_sentiment_vector', model)\n",
    "        data = validate_predictions(data, 'trend_1d', f'{model}_adjusted_score', 'volatility_5d', model, 'adjusted')\n",
    "        data = validate_predictions(data, 'trend_1d', f'{model}_raw_score', 'volatility_5d', model, 'raw')\n",
    "\n",
    "    data.to_csv(f'/tmp/pycharm_project_520/src/data/output/{ticker}_sentiment.csv', index=False)\n",
    "\n",
    "    print(f\"DeBERTa Accuracy:\\nRaw: {(sum((data['DeBERTa_raw_predictions'])) / len(data.index))},\\tAdjusted: {(sum((data['DeBERTa_adjusted_predictions'])) / len(data.index))}\")\n",
    "    print(f\"FinBERT Accuracy:\\nRaw: {(sum((data['FinBERT_raw_predictions'])) / len(data.index))},\\tAdjusted: {(sum((data['FinBERT_adjusted_predictions'])) / len(data.index))}\")\n",
    "    print(f\"DistRoBERTa Accuracy:\\nRaw: {(sum((data['DistRoBERTa_raw_predictions'])) / len(data.index))},\\tAdjusted: {(sum((data['DistRoBERTa_adjusted_predictions'])) / len(data.index))}\")\n",
    "\n",
    "    return data"
   ],
   "id": "60217da93394923f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adatok vizualizálása",
   "id": "9deddbaaf58d0450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_sentiment_plot(df, data_name, size=4):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Define color schemes for each model: (true_color, false_color)\n",
    "    color_schemes = {\n",
    "        'DistRoBERTa': ('#2ecc71', '#a9dfbf'),  # Green shades\n",
    "        'FinBERT': ('#3498db', '#aed6f1'),      # Blue shades\n",
    "        'DeBERTa': ('#9b59b6', '#d7bde2')       # Purple shades\n",
    "    }\n",
    "\n",
    "    sentiment_models = [\n",
    "        ('DistRoBERTa', 'DistRoBERTa_adjusted_score', 'DistRoBERTa_predictions'),\n",
    "        ('FinBERT', 'FinBERT_adjusted_score', 'FinBERT_predictions'),\n",
    "        ('DeBERTa', 'DeBERTa_adjusted_score', 'DeBERTa_predictions')\n",
    "    ]\n",
    "\n",
    "    for model_name, score_col, pred_col in sentiment_models:\n",
    "        # Get color scheme for this model\n",
    "        true_color, false_color = color_schemes[model_name]\n",
    "\n",
    "        # Create scatter plot for each prediction value (0 and 1)\n",
    "        for pred_value in [0, 1]:\n",
    "            mask = df[pred_col] == pred_value\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df[mask]['Date'],\n",
    "                    y=df[mask][score_col],\n",
    "                    name=f'{model_name} (Prediction={pred_value.__bool__()})',\n",
    "                    mode='markers',  # Removed 'lines' to show only markers\n",
    "                    marker=dict(\n",
    "                        color=true_color if pred_value == 1 else false_color,\n",
    "                        size=10,\n",
    "                        symbol='circle'\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'{data_name} Sentiment Scores Over Time by Model',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Sentiment Score',\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True)\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=size))\n",
    "    return fig"
   ],
   "id": "59de7913cab099b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_sentiment_and_stock(data, model_name, ticker):\n",
    "    \"\"\"\n",
    "    Plot sentiment scores and stock price changes over time.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing sentiment and stock data.\n",
    "    - model_name: String, name of the sentiment analysis model (e.g., 'DeBERTa').\n",
    "    \"\"\"\n",
    "\n",
    "    plot_data = data.copy()\n",
    "\n",
    "    plot_data.dropna(inplace=True)\n",
    "    plot_data.dropna(inplace=True)\n",
    "\n",
    "    something_unique, ax1 = plt.subplots(figsize=(22, 6))\n",
    "\n",
    "    # Plot adjusted sentiment score moving average\n",
    "    adjusted_score_col = f\"{model_name}_adjusted_score_ma\"\n",
    "    ax1.plot(plot_data.index, plot_data[adjusted_score_col], color='blue', label=f'{model_name} Adjusted Score MA')\n",
    "    ax1.set_title(f\"{ticker}'s {model_name} Sentiment Scores Over Time by Model\")\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Adjusted Sentiment Score', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%y-%m-%d'))\n",
    "\n",
    "    # Add stock price change on secondary y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(plot_data.index, plot_data['close_diff'], color='red', label='Close Price Change')\n",
    "    ax2.set_ylabel('Price Change', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Add raw sentiment score moving average on another y-axis\n",
    "    ax3 = ax1.twinx()\n",
    "    raw_score_col = f\"{model_name}_raw_score_ma\"\n",
    "    ax3.plot(plot_data.index, plot_data[raw_score_col], color='orange', label=f'{model_name} Raw Score MA')\n",
    "    ax3.set_ylabel('Raw Sentiment Score', color='orange')\n",
    "    ax3.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, loc='upper left', fontsize=12)\n",
    "\n",
    "    plt.show()"
   ],
   "id": "c391c16dbf5c3d01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def organize_data_for_plot(data, models=('DistRoBERTa', 'FinBERT', 'DeBERTa'), stock_window=120, sentiment_window=90, resample='D'):\n",
    "    \"\"\"\n",
    "    Organizes the data for plotting.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data to organize.\n",
    "    - models: Tuple of model names to process.\n",
    "    - stock_window: Rolling window size for stock prices.\n",
    "    - sentiment_window: Rolling window size for sentiment scores.\n",
    "    - resample: Resampling frequency (e.g., 'D' for daily).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame organized for plotting.\n",
    "    \"\"\"\n",
    "    # Create a copy of the data\n",
    "    plot_data = data.copy()\n",
    "\n",
    "    # Ensure the 'Date' column is converted to datetime if it exists\n",
    "    if 'Date' in plot_data.columns:\n",
    "        plot_data['Date'] = pd.to_datetime(plot_data['Date'], errors='coerce')\n",
    "        plot_data.set_index('Date', inplace=True)  # Set 'Date' as the index\n",
    "    else:\n",
    "        raise ValueError(\"The DataFrame must contain a 'Date' column.\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    plot_data.drop(columns=['High', 'Low', 'Open', 'Volume', 'Adj Close', 'News', 'log_returns'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop model-specific columns\n",
    "    for model in models:\n",
    "        model_columns = [f'{model}_sentiment_scores', f'{model}_sentiment_vector', f'{model}_predictions']\n",
    "        plot_data.drop(columns=model_columns, inplace=True, errors='ignore')\n",
    "\n",
    "    # Resample the data and interpolate missing values\n",
    "    plot_data = plot_data.resample(resample).mean()\n",
    "    plot_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    # Calculate rolling averages for sentiment scores\n",
    "    for model in models:\n",
    "        if f'{model}_adjusted_score' in plot_data.columns:\n",
    "            plot_data[f'{model}_adjusted_score_ma'] = plot_data[f'{model}_adjusted_score'].rolling(window=sentiment_window, center=False).mean()\n",
    "        if f'{model}_raw_score' in plot_data.columns:\n",
    "            plot_data[f'{model}_raw_score_ma'] = plot_data[f'{model}_raw_score'].rolling(window=sentiment_window, center=False).mean()\n",
    "\n",
    "    # Calculate rolling averages and differences for stock prices\n",
    "    if 'Close' in plot_data.columns:\n",
    "        plot_data['close_ma'] = plot_data['Close'].rolling(window=stock_window, center=False).mean()\n",
    "        plot_data['close_diff'] = plot_data['Close'] - plot_data['close_ma']\n",
    "\n",
    "    return plot_data"
   ],
   "id": "47e8265f3008a8c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visszatesztelés",
   "id": "dcfaa1a923e464c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_sentiment_data(data, model, score_type, window, resample):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the sentiment data.\n",
    "    - model: Name of the model.\n",
    "    - score_type: Type of sentiment score (e.g., 'adjusted', 'raw').\n",
    "    - window: Size of the rolling window for calculating the mean and rolling average.\n",
    "    - resample: Resampling frequency (e.g., 'D' for daily).\n",
    "\n",
    "    Returns:\n",
    "    - processed_df: DataFrame with processed sentiment data.\n",
    "    \"\"\"\n",
    "    # Convert the string representation of list to actual list and extract positive sentiment\n",
    "    df = data.copy()\n",
    "    df['sentiment'] = df[f'{model}_{score_type}_score']\n",
    "\n",
    "    # Select and rename required columns\n",
    "    processed_df = df.loc[:, ['Date', 'Close', 'High', 'Low', 'Open', 'Volume', 'sentiment']]\n",
    "    processed_df['Date'] = pd.to_datetime(processed_df['Date'])\n",
    "    processed_df.set_index('Date', inplace=True)\n",
    "    processed_df['sentiment'] = processed_df['sentiment'].resample(resample).mean()\n",
    "    processed_df['sentiment'] = processed_df['sentiment'].rolling(window=window, center=False).mean()\n",
    "    processed_df.interpolate(method='linear', inplace=True)\n",
    "    processed_df.fillna(0, inplace=True)\n",
    "    return processed_df"
   ],
   "id": "dca03e6fdb49cd29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PandasSent(bt.feeds.PandasData):\n",
    "    \"\"\"\n",
    "    Custom PandasData feed for backtesting.\n",
    "    \"\"\"\n",
    "    lines = ('sentiment',)\n",
    "    params = (\n",
    "        ('datetime', None),\n",
    "        ('open', 'Open'),\n",
    "        ('high', 'High'),\n",
    "        ('low', 'Low'),\n",
    "        ('close', 'Close'),\n",
    "        ('volume', 'Volume'),\n",
    "        ('sentiment', 'sentiment'),\n",
    "    )"
   ],
   "id": "f7380c63a68d9e59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_backtest(df, model='FinBERT', score_type='raw', window=5, resample='D'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the sentiment data.\n",
    "    - model: Name of the model.\n",
    "    - score_type: Type of sentiment score (e.g., 'adjusted', 'raw').\n",
    "    - window: Size of the rolling window for calculating the mean and rolling average.\n",
    "    - resample: Resampling frequency (e.g., 'D' for daily). Default is 'D'.\n",
    "\n",
    "    Returns:\n",
    "    - results: Backtest results.\n",
    "    - cerebro: Backtesting cerebro.\n",
    "    \"\"\"\n",
    "    processed_data = process_sentiment_data(df, model, score_type, window, resample)\n",
    "    data_feed = PandasSent(dataname=processed_data)\n",
    "    cerebro = bt.Cerebro()\n",
    "    cerebro.addstrategy(SentimentStrategy)\n",
    "    cerebro.adddata(data_feed)\n",
    "    cerebro.broker.setcash(100000.0)\n",
    "    cerebro.broker.setcommission(commission=0.01)  # 0.1% commission\n",
    "    cerebro.addanalyzer(bt.analyzers.PyFolio, _name='PyFolio')\n",
    "    start_value = cerebro.broker.getvalue()\n",
    "    results = cerebro.run()\n",
    "    final_value = cerebro.broker.getvalue()\n",
    "    print('Starting Portfolio Value: %.2f' % start_value)\n",
    "    print('Final Portfolio Value: %.2f' % final_value)\n",
    "    print('Return: %.2f%%' % ((final_value - start_value) / start_value * 100))\n",
    "\n",
    "    return results, cerebro"
   ],
   "id": "f2b06bf5a3a90d40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SentimentStrategy(bt.Strategy):\n",
    "    \"\"\"\n",
    "    Custom strategy for backtesting.\n",
    "    \"\"\"\n",
    "    params = (\n",
    "        ('exitbars', 7),\n",
    "    )\n",
    "\n",
    "    def log(self, txt, dt=None):\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print(f'{dt.isoformat()}: {txt}')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataclose = self.datas[0].close\n",
    "        self.datasentiment = self.datas[0].sentiment\n",
    "        self.order = None\n",
    "        self.buyprice = None\n",
    "        self.buycomm = None\n",
    "\n",
    "    def notify_order(self, order):\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            return\n",
    "\n",
    "        if order.status in [order.Completed]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    f\"BUY EXECUTED, Price: {order.executed.price:.2f}, \"\n",
    "                    f\"Cost: {order.executed.value:.2f}, \"\n",
    "                    f\"Comm: {order.executed.comm:.2f}\"\n",
    "                )\n",
    "                self.buyprice = order.executed.price\n",
    "                self.buycomm = order.executed.comm\n",
    "            else:\n",
    "                self.log(\n",
    "                    f\"SELL EXECUTED, Price: {order.executed.price:.2f}, \"\n",
    "                    f\"Cost: {order.executed.value:.2f}, \"\n",
    "                    f\"Comm: {order.executed.comm:.2f}\"\n",
    "                )\n",
    "\n",
    "            self.bar_executed = len(self)\n",
    "\n",
    "        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n",
    "            self.log(f\"Order Canceled/Margin/Rejected\")\n",
    "\n",
    "        self.order = None\n",
    "\n",
    "    def notify_trade(self, trade):\n",
    "        if not trade.isclosed:\n",
    "            return\n",
    "\n",
    "        self.log(f\"OPERATION PROFIT, GROSS: {trade.pnl:.2f}, NET: {trade.pnlcomm:.2f}\")\n",
    "\n",
    "    def next(self):\n",
    "        self.log(f\"Close: {self.dataclose[0]:.2f}, Sentiment: {self.datasentiment[0]:.3f}\")\n",
    "\n",
    "        if self.order:\n",
    "            return\n",
    "\n",
    "        if not self.position:\n",
    "            # Modified condition to use positive sentiment as buy signal\n",
    "            if self.datasentiment[0] > 0.1:\n",
    "                self.log(f'BUY CREATE, {self.dataclose[0]:.2f}')\n",
    "                self.order = self.buy(size=self.datasentiment[0] * 10000)\n",
    "            if self.datasentiment[0] < -6:\n",
    "                self.log(f'BUY CREATE, {self.dataclose[0]:.2f}')\n",
    "                self.order = self.buy(size=10000)\n",
    "\n",
    "        else:\n",
    "            if self.datasentiment[0] < -1 and len(self) >= (self.bar_executed + self.params.exitbars):\n",
    "                self.log(f'SELL CREATE, {self.dataclose[0]:.2f}')\n",
    "                self.order = self.sell()"
   ],
   "id": "c60757d83274ca0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomPlotter:\n",
    "    \"\"\"\n",
    "    Custom plotter for backtesting.\n",
    "    \"\"\"\n",
    "    def __init__(self, cerebro_instance):\n",
    "        self.cerebro = cerebro_instance\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"\n",
    "        Plot the cerebro trading results including:\n",
    "        - Price and sentiment data\n",
    "        - Portfolio value over time\n",
    "        \"\"\"\n",
    "        # Get the first strategy from the cerebro\n",
    "        strategy = self.cerebro.runstrats[0][0]\n",
    "\n",
    "        # Create figure and axis objects with a single subplot\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[2, 1])\n",
    "\n",
    "        # Get the data from the data feed\n",
    "        datas = strategy.datas[0]\n",
    "\n",
    "        # Convert dates to list for x-axis\n",
    "        dates = [bt.num2date(x) for x in datas.lines.datetime.plot()]\n",
    "\n",
    "        # Plot close prices\n",
    "        ax1.plot(dates, datas.lines.close.plot(), label='Close Price', color='blue')\n",
    "        ax1.set_title('MSFT Price and Trading Activity')\n",
    "        ax1.set_ylabel('Price')\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot sentiment scores if available\n",
    "        if hasattr(datas.lines, 'deberta_sentiment_scores'):\n",
    "            ax2.plot(dates, datas.lines.deberta_sentiment_scores.plot(),\n",
    "                    label='Sentiment Score', color='green')\n",
    "            ax2.set_title('Sentiment Scores')\n",
    "            ax2.set_ylabel('Score')\n",
    "            ax2.grid(True)\n",
    "            ax2.legend()\n",
    "\n",
    "        # Format x-axis\n",
    "        for ax in [ax1, ax2]:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Add buy/sell markers if available in the strategy\n",
    "        if hasattr(strategy, 'buy_points') and hasattr(strategy, 'sell_points'):\n",
    "            for point in strategy.buy_points:\n",
    "                ax1.plot(bt.num2date(point[0]), point[1], '^',\n",
    "                        color='green', markersize=10, label='Buy')\n",
    "            for point in strategy.sell_points:\n",
    "                ax1.plot(bt.num2date(point[0]), point[1], 'v',\n",
    "                        color='red', markersize=10, label='Sell')\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ],
   "id": "b8f35106f7b4ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_backtest(cerebro_instance):\n",
    "    \"\"\"\n",
    "    Wrapper function to create and show the plot\n",
    "    \"\"\"\n",
    "    plotter = CustomPlotter(cerebro_instance)\n",
    "    fig = plotter.plot_results()\n",
    "    return fig"
   ],
   "id": "1d2db0f54054ba55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sentiment Analízis",
   "id": "4a1804b60e6e3939"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NVDA",
   "id": "9638a122df0d0612"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "99b124824bb7b8f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"NVDA\")",
   "id": "77900b2208bd9481",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "1001cd24bfd47ff5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "2f66d2741a4034dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "97b4f4b41bc764ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "f9fce0c8bd8c9a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_plot_data = organize_data_for_plot(NVDA_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "c85c30d105918b9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(NVDA_plot_data, 'DeBERTa', 'NVDA')",
   "id": "5d974377f939d755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(NVDA_plot_data, 'FinBERT', 'NVDA')",
   "id": "4078086df4eafba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(NVDA_plot_data, 'DistRoBERTa', 'NVDA')",
   "id": "a5777b61305f6f08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "b787f0339ad7bbf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=5, center=False).mean().corr(method='spearman')",
   "id": "7760fa7446e34611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=5, center=False).mean().corr()",
   "id": "31ee1f69b253a114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=5, center=False).mean().corr(method='spearman')",
   "id": "ac16f571e31327e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "NVDA_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=5, center=False).mean().corr()",
   "id": "e7988a1bd1aee231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### O",
   "id": "ba523c1eccf40994"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "597f5702b3696150"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"O\")",
   "id": "1141176af6a91417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "b2032751d68a829e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "ae33e56df6a6a2e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "dd3e2754ab20c96c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "da1895a12b2d4ea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_plot_data = organize_data_for_plot(O_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "2f034c198ea26f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(O_plot_data, 'DeBERTa', 'O')",
   "id": "852a6d68387b7355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(O_plot_data, 'FinBERT', 'O')",
   "id": "7126294f0603296d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(O_plot_data, 'DistRoBERTa', 'O')",
   "id": "f46048cd3737113d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "296011839aefcc89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "bc6e9efc54ec836e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "85dbf755c2d22987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "2cd3c5b0422ae230",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "O_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "e29a30dccb6e2b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MU",
   "id": "f47d7b5ffac0b100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "a9212561dced8a04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"MU\")",
   "id": "d4df0139519ae4b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "22148d2268bfe67d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "393f9241918d10fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "38bcd1664c287b3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "afa15e55d37749c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_plot_data = organize_data_for_plot(MU_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "d932381eb7681fcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MU_plot_data, 'DeBERTa', 'MU')",
   "id": "c80b7c10839d969a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MU_plot_data, 'FinBERT', 'MU')",
   "id": "9761af92b682b7b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MU_plot_data, 'DistRoBERTa', 'MU')",
   "id": "78ed9ec836b887e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "f72ffa2c2e749050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "17585cbc46b83da6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "a56c72ad5a8a00de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "734b96dd3ba885f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "ad773c62f821877e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MS",
   "id": "9c8f7d863c28a151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "16352fc89da3b3ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"MS\")",
   "id": "6efc647506ae359f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "1ae1040180266fd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "eeeb64bd5685c718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "8cf6050b7e970a0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "fe921813f9c74d0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_plot_data = organize_data_for_plot(MS_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "9457ef7d44a49863",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MS_plot_data, 'DeBERTa', 'MS')",
   "id": "5d430ec590aa12e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MS_plot_data, 'FinBERT', 'MS')",
   "id": "6c22064a53c140e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MS_plot_data, 'DistRoBERTa', 'MS')",
   "id": "9b3014313c27553e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "b0d9d996d29925da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "63e81c24f54b6555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "8a117a43daa3cb9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "7160efdcba338dd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MS_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "e63c6c8abed23ada",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GOOG",
   "id": "c5ef578d26266329"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "c10e2712b0d6e16d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"GOOG\")",
   "id": "4231029ffbc1c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "e8bb5ea15b3c81fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "d9a08235db4cba67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "27f820eac69b4a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "d096afa5820f15b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_plot_data = organize_data_for_plot(GOOG_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "186fb49df5a196d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(GOOG_plot_data, 'DeBERTa', 'GOOG')",
   "id": "2bf7130a6a88eeba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(GOOG_plot_data, 'FinBERT', 'GOOG')",
   "id": "138ebb9dec3da4c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(GOOG_plot_data, 'DistRoBERTa', 'GOOG')",
   "id": "25a53278af57d83e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "dc564022828f8311"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "804be98b723e7b16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "50fdf16b3b26ef04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "73f7d2b09df62391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "GOOG_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "df7c2074586492e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MRK",
   "id": "ecf2efedfdef0940"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "a4ed9559f05ba3e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"MRK\")",
   "id": "f8872cceb7333842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "eeb4d674edcac4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "90e8bee9ebb81334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "adb21fb12bd358ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "d218b6391b1f5d5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_plot_data = organize_data_for_plot(MRK_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "48eac637fade79c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MRK_plot_data, 'DeBERTa', 'MRK')",
   "id": "e06793b1ccefa208",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MRK_plot_data, 'FinBERT', 'MRK')",
   "id": "9d8f7458d54559bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MRK_plot_data, 'DistRoBERTa', 'MRK')",
   "id": "280b6679abad9459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "7dd519d7c7c5cba8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "5711c1eca1e83a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "fccba01b44f0b2ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "a57cb77d68b1ca71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MRK_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "373f8b6086e817c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### QQQ",
   "id": "8500ae6eb5d45560"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "c5d0a263214e6a2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/analyst_ratings_processed.csv\", ticker=\"QQQ\")",
   "id": "23d4e67a695557c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "d3eb13e7b6bcc863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "8b37372bf74c8fbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "85b7ed42af5c4630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "9292b4a2b662f4e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_plot_data = organize_data_for_plot(QQQ_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "784f78b4839f8e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(QQQ_plot_data, 'DeBERTa', 'QQQ')",
   "id": "b891ab0b9486fa9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(QQQ_plot_data, 'FinBERT', 'QQQ')",
   "id": "2390d6ff7232db7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(QQQ_plot_data, 'DistRoBERTa', 'QQQ')",
   "id": "3749f22b98bba2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "a8ad2f2057f359c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "3c2e43d5b2746040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "459a5b96a4f1c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "f428275b0dde88bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "QQQ_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "3cce969300368811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MSFT",
   "id": "18360eb1dddaf074"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "5c8a078c2d230f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/MicrosoftNews.csv\", ticker=\"MSFT\")",
   "id": "a08e925bd06fb63b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "d8065e35dd6b5c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "aa71beae9e524e82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "ad11a3ad43bee9df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "6133c8e703ad701d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_plot_data = organize_data_for_plot(MSFT_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "4aa61d34637d14c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MSFT_plot_data, 'DeBERTa', 'MSFT')",
   "id": "a71f58e020152c7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MSFT_plot_data, 'FinBERT', 'MSFT')",
   "id": "9ced7570c980b927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(MSFT_plot_data, 'DistRoBERTa', 'MSFT')",
   "id": "19e0d259360db1fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "663dc2be9ca9c8ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "39688cb147e667b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "9a226ec77c87097c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "6f94ae2767507beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MSFT_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "b5449fdb31d85ea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AAPL",
   "id": "717cc25aacb832a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Process Data ####",
   "id": "57f8d7168d98ff5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data = process_data(\"/tmp/pycharm_project_520/src/data/input/raw/source/AAPL.csv\", ticker=\"AAPL\")",
   "id": "60d51fc93122e9e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Data ####",
   "id": "9ad0a9083a1b2f2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data.plot.line(y=\"Close\", x='Date', figsize=(22, 6))",
   "id": "1eccf996e803d4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data.plot.line(y=\"trend_1d\", x='Date', figsize=(22, 6))",
   "id": "b055ff16a8be82f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data.plot.line(y=\"volatility_5d\", x='Date', figsize=(22, 6))",
   "id": "4cc5eca4472f367c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_plot_data = organize_data_for_plot(AAPL_data, resample='D', sentiment_window=3, stock_window=3)",
   "id": "d63c8fcc1b41b261",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(AAPL_plot_data, 'DeBERTa', 'AAPL')",
   "id": "1261d42dc6ef41d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(AAPL_plot_data, 'FinBERT', 'AAPL')",
   "id": "d073c27afcb28732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_sentiment_and_stock(AAPL_plot_data, 'DistRoBERTa', 'AAPL')",
   "id": "5c4e2aaa3d42169d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation ####",
   "id": "b18dff1312d8454d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "bfedbeaaff7132ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data[['DeBERTa_adjusted_score', 'FinBERT_adjusted_score', 'DistRoBERTa_adjusted_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "49e1d5bf9a0927c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr(method='spearman')",
   "id": "4a155dcbb03f49f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "AAPL_data[['DeBERTa_raw_score', 'FinBERT_raw_score', 'DistRoBERTa_raw_score', 'trend_1d']].rolling(window=28, center=False).mean().corr()",
   "id": "e6a0f9ebc9107566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TO DO ##\n",
    "\n",
    "Histogram Equalization\n",
    "3d megbízhatóság számolás"
   ],
   "id": "6c5c9f01ceef72f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot Sentiment Scores ###",
   "id": "926a5ee06721d845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aapl_fig = create_sentiment_plot(AAPL_data, \"Apple\", size=4)\n",
    "aapl_fig.show()"
   ],
   "id": "92072c55c624fcf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "msft_fig = create_sentiment_plot(MSFT_data, \"Microsoft\", size=4)\n",
    "msft_fig.show()"
   ],
   "id": "a8266ab1fc4617a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visszatesztelés",
   "id": "865cc0ab7674b37d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "MU_data = pd.read_csv('/tmp/pycharm_project_520/src/data/output/MU_sentiment.csv')",
   "id": "eb73edd56c65af50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results, cerebro = run_backtest(MS_data, 'DeBERTa', 'adjusted', window=30, resample='D')",
   "id": "889ce522db929427",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plot_backtest(cerebro)\n",
    "plt.show()"
   ],
   "id": "4f4ec51cdc3d8ab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Összefoglalás",
   "id": "dac6b6d9538808e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summary",
   "id": "3a77e30edaa66de5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Irodalomjegyzék",
   "id": "ae989507d27b1954"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
